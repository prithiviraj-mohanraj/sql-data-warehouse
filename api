validation/
├── input/
│   └── schemas.txt
├── scripts/
│   └── extract_tables.sh
├── output/
└── logs/


scripts/extract_tables.sh
#!/bin/bash
set -e

SCHEMA_FILE="../input/schemas.txt"
SQL_FILE="../logs/show_tables.sql"
RAW_OUT="../logs/show_tables_raw.txt"
OUT_TABLES="../output/schema_tables.csv"
OUT_COUNTS="../output/schema_table_counts.csv"

mkdir -p ../output ../logs

echo "schema,table" > $OUT_TABLES
echo "schema,total_tables" > $OUT_COUNTS
> $SQL_FILE

# --------------------------------------------
# Step 1: Generate SQL with markers
# --------------------------------------------
while read SCHEMA
do
    echo "SELECT 'START:${SCHEMA}';" >> $SQL_FILE
    echo "SHOW TABLES IN ${SCHEMA};" >> $SQL_FILE
    echo "SELECT 'END:${SCHEMA}';" >> $SQL_FILE
done < $SCHEMA_FILE

# --------------------------------------------
# Step 2: Run in ONE Impala session
# --------------------------------------------
impala-shell -k -i your-host -f $SQL_FILE -B > $RAW_OUT

# --------------------------------------------
# Step 3: Parse output
# --------------------------------------------
CURRENT_SCHEMA=""
TABLE_COUNT=0

while read LINE
do
    # Detect schema start
    if [[ "$LINE" == START:* ]]; then
        CURRENT_SCHEMA=${LINE#START:}
        TABLE_COUNT=0
        continue
    fi

    # Detect schema end → write count
    if [[ "$LINE" == END:* ]]; then
        echo "$CURRENT_SCHEMA,$TABLE_COUNT" >> $OUT_COUNTS
        CURRENT_SCHEMA=""
        continue
    fi

    # Capture table names
    if [[ -n "$CURRENT_SCHEMA" && -n "$LINE" ]]; then
        echo "$CURRENT_SCHEMA,$LINE" >> $OUT_TABLES
        ((TABLE_COUNT++))
    fi

done < $RAW_OUT

echo "Done."
echo "Tables file → $OUT_TABLES"
echo "Counts file → $OUT_COUNTS"


scripts/extract_columns.sh
#!/bin/bash
set -e

INPUT="../output/schema_tables.csv"
SQL_FILE="../logs/describe_tables.sql"
RAW_OUT="../logs/describe_raw.txt"
OUT="../output/table_columns.csv"

mkdir -p ../output ../logs

echo "schema,table,column,data_type" > $OUT
> $SQL_FILE

# ---------------------------------------------------
# Step 1: Generate SQL with table markers
# ---------------------------------------------------
tail -n +2 $INPUT | while IFS=',' read SCHEMA TABLE
do
    echo "SELECT 'START:${SCHEMA}.${TABLE}';" >> $SQL_FILE
    echo "DESCRIBE ${SCHEMA}.${TABLE};" >> $SQL_FILE
    echo "SELECT 'END:${SCHEMA}.${TABLE}';" >> $SQL_FILE
done

# ---------------------------------------------------
# Step 2: Run once (single connection)
# ---------------------------------------------------
impala-shell -k -i your-host -f $SQL_FILE -B > $RAW_OUT

# ---------------------------------------------------
# Step 3: Parse output safely
# ---------------------------------------------------
CURRENT_SCHEMA=""
CURRENT_TABLE=""

while read LINE
do
    LINE=$(echo "$LINE" | xargs)

    # Detect table start
    if [[ "$LINE" == START:* ]]; then
        TABLE_REF=${LINE#START:}
        CURRENT_SCHEMA=${TABLE_REF%%.*}
        CURRENT_TABLE=${TABLE_REF##*.}
        continue
    fi

    # Detect table end
    if [[ "$LINE" == END:* ]]; then
        CURRENT_SCHEMA=""
        CURRENT_TABLE=""
        continue
    fi

    # Skip headers and empty lines
    if [[ -z "$LINE" || "$LINE" == col_name* || "$LINE" == \#* ]]; then
        continue
    fi

    # Extract column & datatype
    if [[ -n "$CURRENT_SCHEMA" ]]; then
        COLUMN=$(echo "$LINE" | awk '{print $1}')
        DATATYPE=$(echo "$LINE" | awk '{print $2}')

        echo "$CURRENT_SCHEMA,$CURRENT_TABLE,$COLUMN,$DATATYPE" >> $OUT
    fi

done < $RAW_OUT

echo "Column extraction completed → $OUT"

scripts/column_counts.sh
#!/bin/bash

INPUT="../output/table_columns.csv"
OUT="../output/column_counts.csv"

echo "schema,table,column_count" > $OUT

tail -n +2 $INPUT | cut -d',' -f1,2 | sort | uniq -c | \
awk '{print $2","$3","$1}' >> $OUT

echo "Column counts → $OUT"
scripts/extract_oldest_businessdat.sh
#!/bin/bash
set -e

INPUT="../output/schema_tables.csv"
SQL_FILE="../logs/min_businessdat.sql"
RAW_OUT="../logs/min_businessdat_raw.txt"
OUT="../output/oldest_businessdat.csv"

mkdir -p ../output ../logs

echo "schema.table,oldest_businessdat" > $OUT
> $SQL_FILE

# ---------------------------------------------------
# Step 1: Generate SQL with markers
# ---------------------------------------------------
tail -n +2 $INPUT | while IFS=',' read SCHEMA TABLE
do
    echo "SELECT 'START:${SCHEMA}.${TABLE}';" >> $SQL_FILE
    echo "SELECT MIN(businessdat) FROM ${SCHEMA}.${TABLE};" >> $SQL_FILE
    echo "SELECT 'END:${SCHEMA}.${TABLE}';" >> $SQL_FILE
done

# ---------------------------------------------------
# Step 2: Run in ONE Impala session
# ---------------------------------------------------
impala-shell -k -i your-host -f $SQL_FILE -B > $RAW_OUT

# ---------------------------------------------------
# Step 3: Parse output
# ---------------------------------------------------
CURRENT_TABLE=""
VALUE=""

while read LINE
do
    LINE=$(echo "$LINE" | xargs)

    if [[ "$LINE" == START:* ]]; then
        CURRENT_TABLE=${LINE#START:}
        continue
    fi

    if [[ "$LINE" == END:* ]]; then
        echo "$CURRENT_TABLE,$VALUE" >> $OUT
        CURRENT_TABLE=""
        VALUE=""
        continue
    fi

    # Capture MIN value
    if [[ -n "$CURRENT_TABLE" && -n "$LINE" ]]; then
        VALUE=$LINE
    fi

done < $RAW_OUT

echo "Oldest businessdat extraction completed → $OUT"



scripts/schema_table_summary.sh

#!/bin/bash

INPUT="../output/schema_tables.csv"
OUT="../output/summary_counts.txt"

SCHEMA_COUNT=$(cut -d',' -f1 $INPUT | tail -n +2 | sort -u | wc -l)
TABLE_COUNT=$(tail -n +2 $INPUT | wc -l)

echo "Total Schemas: $SCHEMA_COUNT" > $OUT
echo "Total Tables: $TABLE_COUNT" >> $OUT

echo "Summary written → $OUT"


input/businessdays.txt
2024-01-01
2024-02-01


output/schema_tables.csv
output/table_columns.csv
input/businessdays.txt


python generate_validation_sql.py
impala-shell -k -i your-host -f ../logs/validation_queries.sql -B > ../logs/validation_raw.txt
python parse_validation.py


generate_validation_sql.py
import csv
import json

columns_file = "../output/table_columns.csv"
dates_file = "../input/businessdays.txt"
sql_file = "../logs/validation_queries.sql"
meta_file = "../logs/selected_columns.json"

numeric_types = ("int", "bigint", "decimal", "double", "float", "smallint", "tinyint")

# Read business days
with open(dates_file) as f:
    dates = [d.strip() for d in f if d.strip()]

date_list = ",".join([f"'{d}'" for d in dates])

tables = {}
selected_map = {}

# Collect numeric columns per table
with open(columns_file) as f:
    reader = csv.DictReader(f)
    for row in reader:
        dtype = row["data_type"].lower()
        if any(dtype.startswith(nt) for nt in numeric_types):
            key = f"{row['schema']}.{row['table']}"
            tables.setdefault(key, []).append(row["column"])

# Generate SQL
with open(sql_file, "w") as out:
    for table, cols in tables.items():
        selected_cols = cols[:2]  # select only first 2 numeric columns
        if not selected_cols:
            continue

        selected_map[table] = selected_cols

        out.write(f"SELECT 'START:{table}';\n")

        sum_expr = ", ".join([f"SUM({c}) AS {c}" for c in selected_cols])

        out.write(f"""
SELECT businessdat, COUNT(*) AS row_count, {sum_expr}
FROM {table}
WHERE businessdat IN ({date_list})
GROUP BY businessdat;
""")

        out.write(f"SELECT 'END:{table}';\n")

# Save selected columns
with open(meta_file, "w") as m:
    json.dump(selected_map, m)

print("SQL generated.")


parse_validation.py

import csv
import json

raw_file = "../logs/validation_raw.txt"
meta_file = "../logs/selected_columns.json"
out_file = "../output/table_validation_metrics.csv"

selected_map = json.load(open(meta_file))

with open(out_file, "w", newline="") as out:
    writer = csv.writer(out)
    writer.writerow(["schema.table", "businessdat", "row_count", "checksum_column", "checksum_value"])

    current_table = None

    for line in open(raw_file):
        line = line.strip()

        if line.startswith("START:"):
            current_table = line.split(":")[1]
            continue

        if line.startswith("END:"):
            current_table = None
            continue

        if current_table and line:
            parts = line.split()
            business_day = parts[0]
            row_count = parts[1]
            values = parts[2:]

            for col, val in zip(selected_map[current_table], values):
                writer.writerow([current_table, business_day, row_count, col, val])

print("Final validation file created.")



