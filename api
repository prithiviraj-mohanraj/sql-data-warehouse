# pyspark_local_run.py
# Works inside VS Code on a locked-down client laptop (no PATH edits needed)

import os
from pyspark.sql import SparkSession

# ðŸ‘‰ If java -version works in terminal, you can comment these 2 lines.
# ðŸ‘‰ Otherwise, set your JDK folder manually here:
java_home = r"C:\Program Files\Java\jdk-17"
os.environ["JAVA_HOME"] = java_home
os.environ["PATH"] = java_home + r"\bin;" + os.environ["PATH"]

# ---- Start Spark locally ----
spark = (
    SparkSession.builder
    .appName("LocalSparkTest")
    .master("local[*]")  # use all local cores
    .config("spark.sql.warehouse.dir", "./warehouse")
    .config("spark.local.dir", "./spark-tmp")
    .getOrCreate()
)

print("âœ… Java and Spark are set up correctly!")
print("Spark version:", spark.version)

# ---- Simple test job ----
df = spark.createDataFrame([(1, "Alice"), (2, "Bob"), (3, "Cathy")], ["id", "name"])
df.show()

count = spark.sparkContext.parallelize(range(1000)).count()
print("RDD count:", count)

spark.stop()
print("âœ… PySpark ran successfully!")
